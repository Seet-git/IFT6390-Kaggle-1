{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmp9PDQ-C-0Z",
        "outputId": "dbf446be-40e8-40ba-f87c-1c201b332616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.0.0\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (2024.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install pymysql\n",
        "!pip install optuna\n",
        "!pip install pytz\n",
        "from google.colab import drive\n",
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_X2yDXmDRGi",
        "outputId": "42509c0a-bec6-4709-da3c-70eda90b88a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/Kaggle-1/')\n",
        "path = '/content/drive/MyDrive/Kaggle-1/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "j_SoluSU3-qv",
        "outputId": "45377f32-4e2e-46a2-8643-27a79eff0b86"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0a00aeae8962>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Kaggle-1/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaives_bayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_low_frequency_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score\n",
        "import optuna\n",
        "\n",
        "import wandb\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Kaggle-1/')\n",
        "path = '/content/drive/MyDrive/Kaggle-1/'\n",
        "\n",
        "from src.NN.models import *\n",
        "from src.NN.utils import *\n",
        "from src.Naives_bayes.preprocessing import remove_low_frequency_v2\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "montreal_timezone = pytz.timezone('America/Montreal')\n",
        "current_time = datetime.now(montreal_timezone).strftime(\"%m/%d-%H:%M:%S\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "def compute_loss(model, data):\n",
        "    total_loss = 0.0\n",
        "    # Mini-Batch\n",
        "    for inputs, labels in data:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Model prediction\n",
        "        pred = model.forward(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model.criterion(pred.view(-1), labels.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data)\n",
        "\n",
        "\n",
        "def infer(model, data, threshold: float):\n",
        "    # Évaluation du modèle\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data:\n",
        "            # Compute the prediction\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute prediction\n",
        "            pred = (torch.sigmoid(outputs).view(-1) > threshold).float()\n",
        "\n",
        "            # Add prediction\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calcul du F1-score\n",
        "    return f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "\n",
        "def fit(model, train_loader, test_loader, epochs, threshold):\n",
        "    model.to(device)\n",
        "    # Loop on all epochs\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        # Mini-Batch\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Reset backward\n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            # Model prediction\n",
        "            pred = model.forward(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = model.criterion(pred.view(-1), labels.view(-1))\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Change weights\n",
        "            model.optimizer.step()\n",
        "\n",
        "        # Compute loss\n",
        "        train_loss = compute_loss(model, train_loader)\n",
        "        test_loss = compute_loss(model, test_loader)\n",
        "\n",
        "        # F1 score\n",
        "        f1_score_train = infer(model, train_loader, threshold)\n",
        "        f1_score_test = infer(model, test_loader, threshold)\n",
        "\n",
        "        wandb.log({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Train loss\": train_loss,\n",
        "            \"Test loss\": test_loss,\n",
        "            \"Train F1-score\": f1_score_train,\n",
        "            \"Test F1-score\": f1_score_test\n",
        "        })\n",
        "\n",
        "\n",
        "def split_dataset(inputs_train: np.array, labels_train: np.array):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and test set\n",
        "    :param inputs_train:\n",
        "    :param labels_train:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Shuffle the dataset\n",
        "    indices = np.arange(len(inputs_train))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    inputs_shuffled = inputs_train[indices]\n",
        "    labels_shuffled = labels_train[indices]\n",
        "\n",
        "    # Get the size of the training set\n",
        "    # train_size = int(np.ceil(0.8 * len(inputs_train)))\n",
        "    train_size = len(inputs_train) - 2355\n",
        "\n",
        "    # Train set\n",
        "    set_train_inputs = inputs_shuffled[0:train_size, :]\n",
        "    set_train_labels = labels_shuffled[0:train_size]\n",
        "\n",
        "    # Test set\n",
        "    set_test_inputs = inputs_shuffled[train_size:, :]\n",
        "    set_test_labels = labels_shuffled[train_size:]\n",
        "\n",
        "    return set_train_inputs, set_train_labels, set_test_inputs, set_test_labels\n",
        "\n",
        "\n",
        "def evaluation(inputs_documents: torch.Tensor, labels_documents: torch.Tensor, batch_size=128, hidden_layer=256,\n",
        "               learning_rate=0.0001, weight_decay=0.01, epochs=10, threshold=0.5,\n",
        "               minority_weight=4.0, optimizer='Adam', balanced=True) -> float:\n",
        "    # Define model\n",
        "    model = MLPClassifier(inputs_documents.shape[1], hidden_layer).to(device)\n",
        "\n",
        "    # Loss function\n",
        "    class_0_count = (labels_documents == 0).sum().item()\n",
        "    class_1_count = labels_documents.sum().item()\n",
        "    value = torch.tensor([(class_0_count / class_1_count) - minority_weight], dtype=torch.float32).to(device)\n",
        "    weight = torch.maximum(torch.tensor(1), value)\n",
        "    model.criterion = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
        "\n",
        "    # Set optimizer\n",
        "    optimizers = {\n",
        "        'Adam': optim.Adam,\n",
        "        'SGD': optim.SGD,\n",
        "        'RMSprop': optim.RMSprop,\n",
        "        'Adagrad': optim.Adagrad,\n",
        "        'Adadelta': optim.Adadelta,\n",
        "        'Nadam': optim.NAdam\n",
        "    }\n",
        "    model.optimizer = optimizers[optimizer](model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Split dataset to train and test\n",
        "    inputs_train, labels_train, inputs_test, labels_test = split_dataset(inputs_documents, labels_documents)\n",
        "\n",
        "    # Train dataset\n",
        "    train_dataset = TensorDataset(inputs_train, labels_train)\n",
        "\n",
        "    # Balanced dataset\n",
        "    if balanced:\n",
        "        sampler = create_balanced_sampler(labels_train.to(device))\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Test dataset\n",
        "    test_dataset = TensorDataset(inputs_test, labels_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Entraînement\n",
        "    fit(model, train_loader, test_loader, epochs, threshold)\n",
        "\n",
        "    # F1 score\n",
        "    f1_score_test = infer(model, test_loader, threshold)\n",
        "    return f1_score_test\n",
        "\n",
        "\n",
        "def bayesian_optimization(trial):\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, step=32)\n",
        "    hidden_layer = trial.suggest_int(\"hidden_layer\", 64, 512, step=64)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
        "    epochs = trial.suggest_int(\"epochs\", 5, 100)\n",
        "    threshold = trial.suggest_float(\"threshold\", 0.1, 0.9)\n",
        "    minority_weight = trial.suggest_float(\"minority_weight\", 1.0, 4.0)\n",
        "    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"Adagrad\", \"RMSprop\", \"Adadelta\",\"Nadam\"])\n",
        "    balanced = trial.suggest_categorical(\"balanced\", [True, False])\n",
        "\n",
        "    wandb.init(project=\"MLP Optimizer\", name=f\"{current_time} - Trial_{trial.number}\", config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"hidden_layer\": hidden_layer,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"epochs\": epochs,\n",
        "        \"threshold\": threshold,\n",
        "        \"minority_weight\": minority_weight,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"balanced\": balanced\n",
        "    })\n",
        "\n",
        "    score = evaluation(\n",
        "        x_tensor.to(device), y_tensor.to(device),\n",
        "        batch_size=batch_size,\n",
        "        hidden_layer=hidden_layer,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        epochs=epochs,\n",
        "        threshold=threshold,\n",
        "        minority_weight=minority_weight,\n",
        "        optimizer=optimizer,\n",
        "        balanced=balanced\n",
        "    )\n",
        "    return score\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    set_seed(1)\n",
        "    inputs_documents = np.load(f'{path}/data/data_train.npy', allow_pickle=True)\n",
        "    labels_documents = pd.read_csv(f'{path}/data/label_train.csv').to_numpy()[:, 1]\n",
        "    test_documents = np.load(f'{path}/data/data_test.npy', allow_pickle=True)\n",
        "\n",
        "    inputs_documents, test_documents = remove_low_frequency_v2(inputs_documents, test_documents, labels_documents,\n",
        "                                                               threshold=0)\n",
        "    x_tensor = torch.tensor(inputs_documents, dtype=torch.float32).to(device)\n",
        "    y_tensor = torch.tensor(labels_documents, dtype=torch.float32).to(device)\n",
        "\n",
        "    # storage_url = \"mysql+pymysql://optuna_seet:%40g3NYkke%2AeAFRs@localhost/optuna_MLP\" # Local\n",
        "    storage_url = \"mysql+pymysql://optuna_seet:%40g3NYkke%2AeAFRs@0.tcp.ngrok.io:17768/optuna_MLP\" # Google colab - server Mysql - grock\n",
        "\n",
        "    # Bayesian optimization\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        storage=storage_url,\n",
        "        study_name=f\"MLP Optimizer - {current_time}\"\n",
        "    )\n",
        "    study.optimize(bayesian_optimization, n_trials=500)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"\\tValue: {trial.value}\")\n",
        "    print(\"\\tParams:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHvGzZzv18zL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}